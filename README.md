## Yufeng Cui 崔玉峰

**Email**: 1229323056@qq.com; yfcui@baai.ac.cm

**Tel**: +86 18800133341

**[Google Scholar](https://scholar.google.com/citations?user=5Ydha2EAAAAJ&hl=en)** 


## About me

I am a researcher at Beijing Academy of Artificial Intelligence (BAAI). 

## Education

+ 2020-2023. Beihang University, Beijing, China, MS student
+ 2016-2020. Shandong University, Jinan, China, Undergrad student


## First & Co-First Author Publications (Core Contribution)

\* Equal Contribution

Xinlong Wang\*, Xiaosong Zhang\*, Zhengxiong Luo\*, Quan Sun\*, **Yufeng Cui**\*, Jinsheng Wang\*, Fan Zhang\*, Yueze Wang\*, Zhen Li\*, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, Zhongyuan Wang. [Emu3: Next-token prediction is all you need](https://arxiv.org/pdf/2409.18869)
[[code](https://github.com/baaivision/Emu3)] [[project](https://emu.baai.ac.cn/about)] 

Haiwen Diao\*, **Yufeng Cui**\*, Xiaotong Li, Yueze Wang, Huchuan Lu, Xinlong Wang. [EVE: Unveiling Encoder-Free Vision-Language Models](https://arxiv.org/pdf/2406.11832). NIPS 2024. [[code](https://github.com/baaivision/EVE)]

Quan Sun\*, **Yufeng Cui**\*, Xiaosong Zhang\*, Fan Zhang\*, Qiying Yu\*, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang. [Emu2: Generative Multimodal Models are In-Context Learners](https://arxiv.org/abs/2312.13286). CVPR 2024. 
[[code](https://github.com/baaivision/Emu)] [[project](https://baaivision.github.io/emu2/)] [[demo](https://huggingface.co/spaces/BAAI/Emu2)]  

Quan Sun\*, Qiying Yu\*, **Yufeng Cui**\*, Fan Zhang\*, Xiaosong Zhang\*, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang. [Emu: Generative Pretraining in Multimodality](https://arxiv.org/abs/2307.05222). ICLR 2024. 
[[code](https://github.com/baaivision/Emu)]  

**Yufeng Cui**\*, Lichen Zhao\*, Feng Liang\*, Yangguang Li, Jing Shao. [Democratizing contrastive language-image pre-training: A clip benchmark of data, model, and supervision](https://arxiv.org/pdf/2203.05796). ICML Workshop. [[code](https://github.com/Sense-GVT/DeCLIP)]

**Yufeng Cui**, Yimei Kang. [Multi-modal gait recognition via effective spatial-temporal feature fusion]([https://ieeexplore.ieee.org/abstract/document/9859928/](https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_Multi-Modal_Gait_Recognition_via_Effective_Spatial-Temporal_Feature_Fusion_CVPR_2023_paper.pdf)). CVPR 2023.

**Yufeng Cui**, Yimei Kang. [GaitTransformer: Multiple-temporal-scale transformer for cross-view gait recognition](https://ieeexplore.ieee.org/abstract/document/9859928/). ICME 2022.

Shuai Wang\*, **Yufeng Cui**\*, Yimei Kang. [Learning Multiple Granularity Features for Unsupervised Person Re-Identification]([https://ieeexplore.ieee.org/abstract/document/9859928/](https://ieeexplore.ieee.org/abstract/document/9859983)). ICME 2022.


## Co-author Publications

Quan Sun\*, Jinsheng Wang\*, Qiying Yu\*, **Yufeng Cui**, Fan Zhang, Xiaosong Zhang, Xinlong Wang. [EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters](https://arxiv.org/pdf/2402.04252). arXiv:2402.04252.

Qiying Yu\*, Quan Sun\*, Xiaosong Zhang, **Yufeng Cui**, Fan Zhang, Yue Cao, Xinlong Wang, Jingjing Liu. [CapsFusion: Rethinking Image-text Data at Scale](https://arxiv.org/abs/2310.20550). CVPR 2024. [[code&data](https://github.com/baaivision/CapsFusion)]  

Yangguang Li\*, Feng Liang\*, Lichen Zhao\*, **Yufeng Cui**, Wanli Ouyang, Jing Shao, Fengwei Yu, Junjie Yan. [Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm](https://arxiv.org/pdf/2110.05208). ICLR 2022. [[code](https://github.com/Sense-GVT/DeCLIP)]

Yangguang Li, Bin Huang, Zeren Chen, **Yufeng Cui**, Feng Liang, Mingzhu Shen, Fenggang Liu, Enze Xie, Lu Sheng, Wanli Ouyang, Jing Shao. [Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline](https://arxiv.org/pdf/2301.12511). TPAMI 2023. [[code](https://github.com/Sense-GVT/Fast-BEV)]


